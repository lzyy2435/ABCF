{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8de535f-753e-4453-bd11-b4da3eae0bad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "读取数据完成\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import re, collections, math, datetime\n",
    "import json\n",
    "import collections\n",
    "\n",
    "import sklearn\n",
    "import sklearn.svm as svm\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import *\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def getDomainFeature(domain: str):\n",
    "    feature = [domain.count('.'), len(domain), sum(c.isdigit() for c in domain), domain.count('-')]\n",
    "\n",
    "    count_c = 0\n",
    "    special_characters = (':', ';', '#', '!', '%', '~', '+', '_', '?', '=', '&', '[', ']')\n",
    "    for c in domain:\n",
    "        if c in special_characters:\n",
    "            count_c = count_c + 1\n",
    "    feature.append(count_c)\n",
    "\n",
    "    s = len(domain)\n",
    "    dd = collections.defaultdict(int)\n",
    "    for c in domain:\n",
    "        dd[c] += 1\n",
    "\n",
    "    # 字符随机性\n",
    "    # H(d) = － ∑lg( P( Xi ) ) * P( Xi )\n",
    "    feature.append(sum(map(lambda value: -(value / s) * math.log2(value / s), dd.values())))\n",
    "\n",
    "    # 元音字母比例\n",
    "    feature.append(sum(map(lambda x: dd[x], ('a', 'e', 'i', 'o', 'u'))) / s)\n",
    "\n",
    "    # 唯一字符数\n",
    "    feature.append(len(dd))\n",
    "\n",
    "    return feature\n",
    "\n",
    "\n",
    "def getIPFeature(ip: str):\n",
    "    if ':' in ip:\n",
    "        x = ip.split(':')\n",
    "        feature = x[0].split('.') + [x[1]]\n",
    "    else:\n",
    "        feature = ip.split('.') + [80]\n",
    "    feature = list(map(lambda x: int(x), feature))\n",
    "    return feature\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ip_pool = []\n",
    "    ip_label = []\n",
    "\n",
    "    domain_pool = []\n",
    "    domain_label = []\n",
    "    url_pool = []\n",
    "    label_pool = []\n",
    "\n",
    "    n = 0\n",
    "    count = 0\n",
    "    path1 = r\"中国大学生服务外包创新创业大赛/网址自动分类识别/train1.csv\"\n",
    "    for _ in csv.reader(open(path1)):\n",
    "        try:\n",
    "            if re.match(r\"^\\d+\\.\\d+\\.\\d+\\.\\d+(:\\d+)?$\", _[0]):\n",
    "                ip_pool.append(getIPFeature(_[0]))\n",
    "                ip_label.append(int(_[1] == '0'))\n",
    "            else:\n",
    "                if count > 30000:\n",
    "                    break\n",
    "                domain_label.append(int(_[1] == '0'))\n",
    "                domain_pool.append(getDomainFeature(_[0]))\n",
    "                url_pool.append(_[0])\n",
    "                if _[1] == '0':\n",
    "                    count+=1\n",
    "        except:\n",
    "            print(_)\n",
    "\n",
    "        n += 1\n",
    "        if n % 100000 == 0:\n",
    "            # break\n",
    "            print(f\"已读取分析{n}个数据\")\n",
    "            \n",
    "            \n",
    "    path2 = \"data1add.csv\"\n",
    "    for _ in csv.reader(open(path2)):\n",
    "        try:\n",
    "            if re.match(r\"^\\d+\\.\\d+\\.\\d+\\.\\d+(:\\d+)?$\", _[0]):\n",
    "                ip_pool.append(getIPFeature(_[0]))\n",
    "                ip_label.append(int(_[1] == '0'))\n",
    "            else:\n",
    "                domain_label.append(int(_[1] == '0'))\n",
    "                domain_pool.append(getDomainFeature(_[0]))\n",
    "                url_pool.append(_[0])\n",
    "        except:\n",
    "            print(_)\n",
    "\n",
    "        n += 1\n",
    "        if n % 100000 == 0:\n",
    "            # break\n",
    "            print(f\"已读取分析{n}个数据\")\n",
    "            \n",
    "    \n",
    "    print(\"读取数据完成\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f505d868-71a9-443a-9517-2368f30c49bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ip_pool = []\n",
    "# ip_label = []\n",
    "\n",
    "# domain_pool = []\n",
    "# domain_label = []\n",
    "# url_pool = []\n",
    "\n",
    "# n = 0\n",
    "# path1 = r\"中国大学生服务外包创新创业大赛/网址自动分类识别/train1.csv\"\n",
    "# for _ in csv.reader(open(path1)):\n",
    "#     try:\n",
    "#         if re.match(r\"^\\d+\\.\\d+\\.\\d+\\.\\d+(:\\d+)?$\", _[0]):\n",
    "#             ip_pool.append(getIPFeature(_[0]))\n",
    "#             ip_label.append(int(_[1] == '0'))\n",
    "#         else:\n",
    "#             domain_label.append(int(_[1] == '0'))\n",
    "#             domain_pool.append(getDomainFeature(_[0]))\n",
    "#             url_pool.append(_[0])\n",
    "#     except:\n",
    "#         print(_)\n",
    "\n",
    "#     n += 1\n",
    "#     if n % 100000 == 0:\n",
    "#         # break\n",
    "#         print(f\"已读取分析{n}个数据\")\n",
    "\n",
    "\n",
    "# path2 = \"data1add.csv\"\n",
    "# for _ in csv.reader(open(path2)):\n",
    "#     try:\n",
    "#         if re.match(r\"^\\d+\\.\\d+\\.\\d+\\.\\d+(:\\d+)?$\", _[0]):\n",
    "#             ip_pool.append(getIPFeature(_[0]))\n",
    "#             ip_label.append(int(_[1] == '0'))\n",
    "#         else:\n",
    "#             domain_label.append(int(_[1] == '0'))\n",
    "#             domain_pool.append(getDomainFeature(_[0]))\n",
    "#             url_pool.append(_[0])\n",
    "#     except:\n",
    "#         print(_)\n",
    "\n",
    "#     n += 1\n",
    "#     if n % 100000 == 0:\n",
    "#         # break\n",
    "#         print(f\"已读取分析{n}个数据\")\n",
    "\n",
    "\n",
    "# print(\"读取数据完成\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa1f17e5-a342-4c24-accd-e6b192b10de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# np.savetxt(\"domain_pool_large.txt\",np.array(domain_pool))\n",
    "# np.savetxt(\"domain_label_large.txt\",np.array(domain_label))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98c5e4e0-9ab2-4927-a839-8174fc499410",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('url_pool.txt', 'w+', encoding='utf-8') as f:\n",
    "    for data in url_pool:\n",
    "        # 添加‘\\n’用于换行\n",
    "        f.write(data+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "619b14f3-941a-4d19-991f-78cfdcd6e772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50105\n",
      "50105\n"
     ]
    }
   ],
   "source": [
    "print(len(url_pool))\n",
    "print(len(domain_pool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67c36069-ea2b-4638-8f97-26714f5f4aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# domain_label = np.loadtxt(\"domain_label.txt\")\n",
    "# domain_pool = np.loadtxt(\"domain_pool.txt\")\n",
    "# print(domain_pool[0])\n",
    "# print(domain_label2==domain_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb5643e5-e5ed-4d4d-a95a-76494bd8457b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_res_(values: list):\n",
    "    value_cnt = {}  # 将结果用一个字典存储\n",
    "    # 统计结果\n",
    "    for value in values:\n",
    "        # get(value, num)函数的作用是获取字典中value对应的键值, num=0指示初始值大小。\n",
    "        value_cnt[value] = value_cnt.get(value, 0) + 1\n",
    "\n",
    "    # 打印输出结果\n",
    "    print(value_cnt)\n",
    "    print([key for key in value_cnt.keys()])\n",
    "    print([value for value in value_cnt.values()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52e4e143-287a-4d93-9153-508afee373d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50105\n",
      "{0: 20104, 1: 30001}\n",
      "[0, 1]\n",
      "[20104, 30001]\n",
      "{0: 539, 1: 675}\n",
      "[0, 1]\n",
      "[539, 675]\n"
     ]
    }
   ],
   "source": [
    "print(len(domain_pool))\n",
    "# print(len([i for i in domain_label=='0'))\n",
    "get_res_(domain_label)\n",
    "get_res_(ip_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e96ca81-3deb-4c58-ba84-3c823adafe10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn.functional as F # 激励函数都在这\n",
    "# # import tqdm\n",
    "# import sklearn\n",
    "\n",
    "# randomSeed = 1\n",
    "# domain_size = len(domain_pool)\n",
    "# domain_pool = preprocessing.scale(domain_pool)\n",
    "# domain_pool, domain_label = sklearn.utils.shuffle(domain_pool, domain_label, random_state=randomSeed)\n",
    "\n",
    "# class Net(torch.nn.Module): # 继承 torch 的 Module\n",
    "#     def __init__(self, n_feature, n_hidden, n_output):\n",
    "#         super(Net, self).__init__() # 继承 __init__ 功能\n",
    "#         self.hidden = torch.nn.Linear(n_feature, n_hidden) # 隐藏层线性输出\n",
    "#         self.hidden1 = torch.nn.Linear(n_hidden, n_hidden*3) # 隐藏层线性输出\n",
    "#         self.hidden2 = torch.nn.Linear(n_hidden*3, n_hidden) # 隐藏层线性输出\n",
    "#         self.hidden3 = torch.nn.Linear(n_hidden, n_hidden) # 隐藏层线性输出\n",
    "#         self.out = torch.nn.Linear(n_hidden, n_output) # 输出层线性输出\n",
    "#     def forward(self, x):\n",
    "#         # 正向传播输入值, 神经网络分析出输出值\n",
    "#         x = F.relu(self.hidden(x)) # 激励函数(隐藏层的线性值)\n",
    "#         x = F.relu(self.hidden1(x))\n",
    "#         x = F.relu(self.hidden2(x))\n",
    "#         x = F.relu(self.hidden3(x))\n",
    "#         x = self.out(x) # 输出值, 但是这个不是预测值, 预测值还需要再另外计算\n",
    "#         return x\n",
    "# net = Net(n_feature=8, n_hidden=10, n_output=2) # 几个类别就几个 output\n",
    "# print(net)\n",
    "\n",
    "# x=torch.Tensor(domain_pool[:int(domain_size * 0.8)]).type(torch.FloatTensor) \n",
    "# y=torch.Tensor(domain_label[:int(domain_size * 0.8)]).type(torch.LongTensor)\n",
    "# x_test=torch.Tensor(domain_pool[int(domain_size * 0.8):]).type(torch.FloatTensor)\n",
    "# y_test=torch.Tensor(domain_label[int(domain_size * 0.8):]).type(torch.LongTensor)\n",
    "\n",
    "# optimizer = torch.optim.SGD(net.parameters(), lr=0.02) #学习率等\n",
    "# loss_func = torch.nn.CrossEntropyLoss() #交叉熵损失函数\n",
    "# # loss_func = focal_loss(alpha=0.25, gamma=2, num_classes=2)\n",
    "# for t in range(100):\n",
    "#     out = net(x) \n",
    "#     loss = loss_func(out, y) # (1. nn output, 2. target)\n",
    "#     optimizer.zero_grad() # 清空梯度\n",
    "#     loss.backward() # 反向\n",
    "#     optimizer.step() # 应用梯度\n",
    "#     print(t,end=\" \")\n",
    "    \n",
    "#     if t%1==0:\n",
    "#         prediction = torch.max(out, 1)[1]\n",
    "#         # print(prediction)\n",
    "        \n",
    "#         pred_y = prediction.data.numpy()\n",
    "#         # print(pred_y)\n",
    "#         label_y = y.data.numpy()\n",
    "\n",
    "#         now = datetime.datetime.now()\n",
    "#         timestamp = datetime.datetime.timestamp(now)\n",
    "#         TF = {\"timestamp\":timestamp,\n",
    "#               \"randomSeed\":randomSeed,\n",
    "#               \"训练集规模\":int(domain_size * 0.8),\n",
    "#               \"测试集规模\":domain_size - int(domain_size * 0.8),\n",
    "#               \"标准化均值\":tuple(domain_pool.mean(axis=0)),\n",
    "#               \"标准化方差\":tuple(domain_pool.std(axis=0)),\n",
    "#               \"准确率\": accuracy_score(label_y, pred_y),\n",
    "#               \"f1\": f1_score(label_y, pred_y),\n",
    "#               \"f1_macro\": f1_score(label_y, pred_y,average=\"macro\"),\n",
    "#               \"recall_macro\": recall_score(label_y, pred_y,average=\"macro\"),\n",
    "#               \"roc_macro\": roc_auc_score(label_y, pred_y,average=\"macro\"),\n",
    "#               \"precision_macro\": precision_score(label_y, pred_y,average=\"macro\"),\n",
    "#               \"recall\": recall_score(label_y, pred_y),\n",
    "#               \"roc\": roc_auc_score(label_y, pred_y),\n",
    "#               \"precision\": precision_score(label_y, pred_y),\n",
    "#               \"模型名称\": \"NN\"\n",
    "#               }\n",
    "\n",
    "#         with open(\"log_\"+TF[\"模型名称\"]+\".json\",\"w\") as f:\n",
    "#             b = json.dumps(TF)\n",
    "#             f.write(b)\n",
    "\n",
    "#         print(f\"测试集\\033[0;34m准确率\\033[0m为\\033[0;31m{TF['准确率'] * 100:.2f}%\")\n",
    "#         print(f\"\\033[0m测试集\\033[0;34mf1值\\033[0m为\\033[0;31m{TF['f1'] * 100:.2f}%\")\n",
    "#         print(f\"\\033[0m测试集\\033[0;34mrecall值\\033[0m为\\033[0;31m{TF['recall'] * 100:.2f}%\")\n",
    "#         print(f\"\\033[0m测试集\\033[0;34mroc值\\033[0m为\\033[0;31m{TF['roc'] * 100:.2f}%\")\n",
    "#         print(f\"\\033[0m测试集\\033[0;34mprecision值\\033[0m为\\033[0;31m{TF['precision'] * 100:.2f}%\\033[0m\")\n",
    "#         print(f\"\\033[0m测试集\\033[0;34mf1_macro值\\033[0m为\\033[0;31m{TF['f1_macro'] * 100:.2f}%\")\n",
    "#         print(f\"\\033[0m测试集\\033[0;34mrecall_macro值\\033[0m为\\033[0;31m{TF['recall_macro'] * 100:.2f}%\")\n",
    "#         print(f\"\\033[0m测试集\\033[0;34mroc_macro值\\033[0m为\\033[0;31m{TF['roc_macro'] * 100:.2f}%\")\n",
    "#         print(f\"\\033[0m测试集\\033[0;34mprecision_macro值\\033[0m为\\033[0;31m{TF['precision_macro'] * 100:.2f}%\\033[0m\")\n",
    "\n",
    "# # prediction = torch.max(out, 1)[1]\n",
    "# # #print(prediction)\n",
    "# # pred_y = prediction.data.numpy()\n",
    "# # target_y = y.data.numpy()\n",
    "# # # plt.scatter(x.data.numpy()[:, 0], x.data.numpy()[:, 1], c=pred_y, s=100, lw=0, cmap='rainbow')\n",
    "# # accuracy = float((pred_y == target_y).astype(int).sum()) / float(target_y.size)\n",
    "\n",
    "# # print( '训练集准确度：',accuracy)\n",
    "# # # plt.show()\n",
    "\n",
    "# # out=net(x_test)\n",
    "# # prediction = torch.max(out, 1)[1]\n",
    "# # pred_y = prediction.data.numpy()\n",
    "# # ta_y = y_test.data.numpy()\n",
    "# # # plt.scatter(x_test.data.numpy()[:, 0], x_test.data.numpy()[:, 1], c=pred_y, s=100, lw=0, cmap='rain\n",
    "# # accuracy = float((pred_y == ta_y).astype(int).sum()) / float(ta_y.size)\n",
    "# # print( '测试集准确度：',accuracy)\n",
    "# # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12679acd-459d-486a-b8aa-107f0f3bd227",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =            9     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  2.77841D+04    |proj g|=  1.65879D+04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " This problem is unconstrained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "    9     35     39      1     0     0   2.737D-02   7.046D+03\n",
      "  F =   7045.7546134826689     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "训练模型完成\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   29.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试集\u001b[0;34m准确率\u001b[0m为\u001b[0;31m93.28%\n",
      "\u001b[0m测试集\u001b[0;34mf1值\u001b[0m为\u001b[0;31m94.26%\n",
      "\u001b[0m测试集\u001b[0;34mrecall值\u001b[0m为\u001b[0;31m91.56%\n",
      "\u001b[0m测试集\u001b[0;34mroc值\u001b[0m为\u001b[0;31m93.72%\n",
      "\u001b[0m测试集\u001b[0;34mprecision值\u001b[0m为\u001b[0;31m97.12%\n"
     ]
    }
   ],
   "source": [
    "domain_svm_model = sklearn.linear_model.LogisticRegression(verbose=True)\n",
    "\n",
    "randomSeed = 1\n",
    "domain_size = len(domain_pool)\n",
    "domain_pool = preprocessing.scale(domain_pool)\n",
    "domain_pool, domain_label = sklearn.utils.shuffle(domain_pool, domain_label, random_state=randomSeed)\n",
    "\n",
    "# 训练模型,x为训练集,y为标签\n",
    "domain_svm_model.fit(domain_pool[:int(domain_size * 0.8)], domain_label[:int(domain_size * 0.8)])\n",
    "print(\"训练模型完成\")\n",
    "\n",
    "pred_y = domain_svm_model.predict(domain_pool[int(domain_size * 0.8):])\n",
    "label_y = domain_label[int(domain_size * 0.8):]\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "timestamp = datetime.datetime.timestamp(now)\n",
    "\n",
    "TF = {\"timestamp\":timestamp,\n",
    "      \"randomSeed\":randomSeed,\n",
    "      \"训练集规模\":int(domain_size * 0.8),\n",
    "      \"测试集规模\":domain_size - int(domain_size * 0.8),\n",
    "      \"标准化均值\":tuple(domain_pool.mean(axis=0)),\n",
    "      \"标准化方差\":tuple(domain_pool.std(axis=0)),\n",
    "      \"准确率\": accuracy_score(label_y, pred_y),\n",
    "      \"f1\": f1_score(label_y, pred_y),\n",
    "      \"recall\": recall_score(label_y, pred_y),\n",
    "      \"roc\": roc_auc_score(label_y, pred_y),\n",
    "      \"precision\": precision_score(label_y, pred_y),\n",
    "      \"模型名称\": \"logistic regression\"\n",
    "      }\n",
    "\n",
    "with open(\"log_\"+TF[\"模型名称\"]+\".json\",\"w\") as f:\n",
    "    b = json.dumps(TF)\n",
    "    f.write(b)\n",
    "\n",
    "print(f\"测试集\\033[0;34m准确率\\033[0m为\\033[0;31m{TF['准确率'] * 100:.2f}%\")\n",
    "print(f\"\\033[0m测试集\\033[0;34mf1值\\033[0m为\\033[0;31m{TF['f1'] * 100:.2f}%\")\n",
    "print(f\"\\033[0m测试集\\033[0;34mrecall值\\033[0m为\\033[0;31m{TF['recall'] * 100:.2f}%\")\n",
    "print(f\"\\033[0m测试集\\033[0;34mroc值\\033[0m为\\033[0;31m{TF['roc'] * 100:.2f}%\")\n",
    "print(f\"\\033[0m测试集\\033[0;34mprecision值\\033[0m为\\033[0;31m{TF['precision'] * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30bc7a90-22b1-45d8-950f-b3120627ef27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['logistic_regression_model.pkl']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(domain_svm_model, 'logistic_regression_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "339fdef3-8270-4a63-a7da-2a1f6b02a92c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 4336, 1: 5685}\n",
      "[0, 1]\n",
      "[4336, 5685]\n",
      "{0: 3991, 1: 6030}\n",
      "[0, 1]\n",
      "[3991, 6030]\n",
      "[0 0 0 ... 0 1 1]\n",
      "{0: 494, 1: 606}\n",
      "[0, 1]\n",
      "[494, 606]\n"
     ]
    }
   ],
   "source": [
    "get_res_(pred_y)\n",
    "get_res_(label_y)\n",
    "print(pred_y[:1100])\n",
    "get_res_(pred_y[:1100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a3726a-a9ac-4920-b4b0-4496c9a016cd",
   "metadata": {},
   "source": [
    "## 对全部数据集train1和train1补充进行过滤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5aad73da-05df-4e50-ab9d-8adcd92579a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已读取分析100000个数据\n",
      "已读取分析200000个数据\n",
      "已读取分析300000个数据\n",
      "已读取分析400000个数据\n",
      "已读取分析500000个数据\n",
      "已读取分析600000个数据\n",
      "已读取分析700000个数据\n",
      "已读取分析800000个数据\n",
      "已读取分析900000个数据\n",
      "已读取分析1000000个数据\n",
      "已读取分析1100000个数据\n",
      "已读取分析1200000个数据\n",
      "已读取分析1300000个数据\n",
      "已读取分析1400000个数据\n",
      "已读取分析1500000个数据\n",
      "已读取分析1600000个数据\n",
      "已读取分析1700000个数据\n",
      "已读取分析1800000个数据\n",
      "已读取分析1900000个数据\n",
      "已读取分析2000000个数据\n",
      "已读取分析2100000个数据\n",
      "已读取分析2200000个数据\n",
      "已读取分析2300000个数据\n",
      "已读取分析2400000个数据\n",
      "已读取分析2500000个数据\n",
      "已读取分析2600000个数据\n",
      "已读取分析2700000个数据\n",
      "已读取分析2800000个数据\n",
      "已读取分析2900000个数据\n",
      "已读取分析3000000个数据\n",
      "已读取分析3100000个数据\n",
      "已读取分析3200000个数据\n",
      "已读取分析3300000个数据\n",
      "已读取分析3400000个数据\n",
      "已读取分析3500000个数据\n",
      "已读取分析3600000个数据\n",
      "已读取分析3700000个数据\n",
      "已读取分析3800000个数据\n",
      "已读取分析3900000个数据\n",
      "已读取分析4000000个数据\n",
      "已读取分析4100000个数据\n",
      "已读取分析4200000个数据\n",
      "已读取分析4300000个数据\n",
      "已读取分析4400000个数据\n",
      "已读取分析4500000个数据\n",
      "已读取分析4600000个数据\n",
      "已读取分析4700000个数据\n",
      "已读取分析4800000个数据\n",
      "已读取分析4900000个数据\n",
      "已读取分析5000000个数据\n",
      "已读取分析5100000个数据\n",
      "已读取分析5200000个数据\n",
      "已读取分析5300000个数据\n",
      "已读取分析5400000个数据\n",
      "已读取分析5500000个数据\n",
      "已读取分析5600000个数据\n",
      "已读取分析5700000个数据\n",
      "已读取分析5800000个数据\n",
      "已读取分析5900000个数据\n",
      "已读取分析6000000个数据\n",
      "已读取分析6100000个数据\n",
      "已读取分析6200000个数据\n",
      "已读取分析6300000个数据\n",
      "已读取分析6400000个数据\n",
      "已读取分析6500000个数据\n",
      "已读取分析6600000个数据\n",
      "已读取分析6700000个数据\n",
      "已读取分析6800000个数据\n",
      "已读取分析6900000个数据\n",
      "已读取分析7000000个数据\n",
      "已读取分析7100000个数据\n",
      "已读取分析7200000个数据\n",
      "已读取分析7300000个数据\n",
      "已读取分析7400000个数据\n",
      "已读取分析7500000个数据\n",
      "已读取分析7600000个数据\n",
      "已读取分析7700000个数据\n",
      "已读取分析7800000个数据\n",
      "已读取分析7900000个数据\n",
      "已读取分析8000000个数据\n",
      "已读取分析8100000个数据\n",
      "已读取分析8200000个数据\n",
      "已读取分析8300000个数据\n",
      "已读取分析8400000个数据\n",
      "已读取分析8500000个数据\n",
      "['16566']\n",
      "读取数据完成\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import re, collections, math, datetime\n",
    "import json\n",
    "import collections\n",
    "\n",
    "import sklearn\n",
    "import sklearn.svm as svm\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import *\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def getDomainFeature(domain: str):\n",
    "    feature = [domain.count('.'), len(domain), sum(c.isdigit() for c in domain), domain.count('-')]\n",
    "\n",
    "    count_c = 0\n",
    "    special_characters = (':', ';', '#', '!', '%', '~', '+', '_', '?', '=', '&', '[', ']')\n",
    "    for c in domain:\n",
    "        if c in special_characters:\n",
    "            count_c = count_c + 1\n",
    "    feature.append(count_c)\n",
    "\n",
    "    s = len(domain)\n",
    "    dd = collections.defaultdict(int)\n",
    "    for c in domain:\n",
    "        dd[c] += 1\n",
    "\n",
    "    # 字符随机性\n",
    "    # H(d) = － ∑lg( P( Xi ) ) * P( Xi )\n",
    "    feature.append(sum(map(lambda value: -(value / s) * math.log2(value / s), dd.values())))\n",
    "\n",
    "    # 元音字母比例\n",
    "    feature.append(sum(map(lambda x: dd[x], ('a', 'e', 'i', 'o', 'u'))) / s)\n",
    "\n",
    "    # 唯一字符数\n",
    "    feature.append(len(dd))\n",
    "\n",
    "    return feature\n",
    "\n",
    "\n",
    "def getIPFeature(ip: str):\n",
    "    if ':' in ip:\n",
    "        x = ip.split(':')\n",
    "        feature = x[0].split('.') + [x[1]]\n",
    "    else:\n",
    "        feature = ip.split('.') + [80]\n",
    "    feature = list(map(lambda x: int(x), feature))\n",
    "    return feature\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ip_pool = []\n",
    "    ip_label = []\n",
    "\n",
    "    domain_pool = []\n",
    "    domain_label = []\n",
    "    url_pool = []\n",
    "    label_pool = []\n",
    "\n",
    "    n = 0\n",
    "    count = 0\n",
    "    path1 = r\"中国大学生服务外包创新创业大赛/网址自动分类识别/train1.csv\"\n",
    "    for _ in csv.reader(open(path1)):\n",
    "        try:\n",
    "            if re.match(r\"^\\d+\\.\\d+\\.\\d+\\.\\d+(:\\d+)?$\", _[0]):\n",
    "                ip_pool.append(getIPFeature(_[0]))\n",
    "                ip_label.append(int(_[1] == '0'))\n",
    "            else:\n",
    "                domain_label.append(int(_[1] == '0'))\n",
    "                domain_pool.append(getDomainFeature(_[0]))\n",
    "                url_pool.append(_[0])\n",
    "                label_pool.append(_[1])\n",
    "                if _[1] == '0':\n",
    "                    count+=1\n",
    "        except:\n",
    "            print(_)\n",
    "\n",
    "        n += 1\n",
    "        if n % 100000 == 0:\n",
    "            # break\n",
    "            print(f\"已读取分析{n}个数据\")\n",
    "            \n",
    "            \n",
    "    path2 = \"data1add.csv\"\n",
    "    for _ in csv.reader(open(path2)):\n",
    "        try:\n",
    "            if re.match(r\"^\\d+\\.\\d+\\.\\d+\\.\\d+(:\\d+)?$\", _[0]):\n",
    "                ip_pool.append(getIPFeature(_[0]))\n",
    "                ip_label.append(int(_[1] == '0'))\n",
    "            else:\n",
    "                domain_label.append(int(_[1] == '0'))\n",
    "                domain_pool.append(getDomainFeature(_[0]))\n",
    "                url_pool.append(_[0])\n",
    "                label_pool.append(_[1])\n",
    "        except:\n",
    "            print(_)\n",
    "\n",
    "        n += 1\n",
    "        if n % 100000 == 0:\n",
    "            # break\n",
    "            print(f\"已读取分析{n}个数据\")\n",
    "    \n",
    "    print(\"读取数据完成\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2b1b416b-9fc4-43a5-a39b-dd76c5a5b1d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8345817\n",
      "8345817\n",
      "8345817\n"
     ]
    }
   ],
   "source": [
    "print(len(domain_pool))\n",
    "print(len(domain_label))\n",
    "print(len(url_pool))\n",
    "print(len(label_pool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2035f260-81f9-4735-8872-1667cd277375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16691634\n"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# domain_label_a =np.loadtxt(\"domain_label_large.txt\")\n",
    "# domain_pool_a = np.loadtxt(\"domain_pool_large.txt\")\n",
    "# url_pool = []\n",
    "# with open('url_pool.txt', 'r', encoding='utf-8') as f:\n",
    "#     for data in f:\n",
    "#         url_pool.append(data[:-1])\n",
    "        \n",
    "# l_pool = []\n",
    "# with open('label_all.txt', 'r', encoding='utf-8') as f:\n",
    "#     for data in f:\n",
    "#         l_pool.append(data[:-1])\n",
    "# print(len(l_pool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a7623ba-0801-4f4b-85d1-6764fb56358f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# l_pool = []\n",
    "# with open('label-all.txt', 'r', encoding='utf-8') as f:\n",
    "#     for data in f:\n",
    "#         l_pool.append(data[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "441015a1-c8bd-4ff2-93b5-c5a461db3956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8345817\n",
      "25037451\n"
     ]
    }
   ],
   "source": [
    "# print(len(l_pool))\n",
    "# print(len(url_pool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "66b5ae5f-88d6-4c71-9009-3ea44139e473",
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_label = np.asarray(domain_label,dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "51040318-3690-4ebe-9d2d-83f396b67e17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试集\u001b[0;34m准确率\u001b[0m为\u001b[0;31m84.51%\n",
      "\u001b[0m测试集\u001b[0;34mf1值\u001b[0m为\u001b[0;31m91.58%\n",
      "\u001b[0m测试集\u001b[0;34mrecall值\u001b[0m为\u001b[0;31m84.47%\n",
      "\u001b[0m测试集\u001b[0;34mprecision值\u001b[0m为\u001b[0;31m100.00%\n"
     ]
    }
   ],
   "source": [
    "domain_pool = preprocessing.scale(domain_pool)\n",
    "\n",
    "pred_y = domain_svm_model.predict(domain_pool)\n",
    "pred_y_p = domain_svm_model.predict_proba(domain_pool)\n",
    "\n",
    "label_y = domain_label\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "timestamp = datetime.datetime.timestamp(now)\n",
    "TF = {\"timestamp\":timestamp,\n",
    "      \"randomSeed\":randomSeed,\n",
    "      \"训练集规模\":int(domain_size * 0.8),\n",
    "      \"测试集规模\":domain_size - int(domain_size * 0.8),\n",
    "      \"标准化均值\":tuple(domain_pool.mean(axis=0)),\n",
    "      \"标准化方差\":tuple(domain_pool.std(axis=0)),\n",
    "      \"准确率\": accuracy_score(label_y, pred_y),\n",
    "      \"f1\": f1_score(label_y, pred_y),\n",
    "      \"recall\": recall_score(label_y, pred_y),\n",
    "      # \"roc\": roc_auc_score(label_y, pred_y),\n",
    "      \"precision\": precision_score(label_y, pred_y),\n",
    "      \"模型名称\":\"logistic regression all-130w-new\"\n",
    "      \n",
    "      }\n",
    "\n",
    "with open(\"log_\"+TF[\"模型名称\"]+\".json\",\"w\") as f:\n",
    "    b = json.dumps(TF)\n",
    "    f.write(b)\n",
    "\n",
    "print(f\"测试集\\033[0;34m准确率\\033[0m为\\033[0;31m{TF['准确率'] * 100:.2f}%\")\n",
    "print(f\"\\033[0m测试集\\033[0;34mf1值\\033[0m为\\033[0;31m{TF['f1'] * 100:.2f}%\")\n",
    "print(f\"\\033[0m测试集\\033[0;34mrecall值\\033[0m为\\033[0;31m{TF['recall'] * 100:.2f}%\")\n",
    "# print(f\"\\033[0m测试集\\033[0;34mroc值\\033[0m为\\033[0;31m{TF['roc'] * 100:.2f}%\")\n",
    "print(f\"\\033[0m测试集\\033[0;34mprecision值\\033[0m为\\033[0;31m{TF['precision'] * 100:.2f}%\")\n",
    "\n",
    "# TF = {\"timestamp\":timestamp,\n",
    "#       \"randomSeed\":randomSeed,\n",
    "#       # \"训练集规模\":int(domain_size * 0.8),\n",
    "#       # \"测试集规模\":domain_size - int(domain_size * 0.8),\n",
    "#       \"标准化均值\":tuple(domain_pool.mean(axis=0)),\n",
    "#       \"标准化方差\":tuple(domain_pool.std(axis=0)),\n",
    "#       \"准确率\": accuracy_score(label_y, pred_y),\n",
    "#       \"f1\": f1_score(label_y, pred_y),\n",
    "#       \"f1_macro\": f1_score(label_y, pred_y,average=\"macro\"),\n",
    "#       \"recall_macro\": recall_score(label_y, pred_y,average=\"macro\"),\n",
    "#       \"roc_macro\": roc_auc_score(label_y, pred_y,average=\"macro\"),\n",
    "#       \"precision_macro\": precision_score(label_y, pred_y,average=\"macro\"),\n",
    "#       \"recall\": recall_score(label_y, pred_y),\n",
    "#       \"roc\": roc_auc_score(label_y, pred_y),\n",
    "#       \"precision\": precision_score(label_y, pred_y),\n",
    "#       \"模型名称\": \"logistic regression all\"\n",
    "#       }\n",
    "\n",
    "# with open(\"log_\"+TF[\"模型名称\"]+\".json\",\"w\") as f:\n",
    "#     b = json.dumps(TF)\n",
    "#     f.write(b)\n",
    "\n",
    "# print(f\"测试集\\033[0;34m准确率\\033[0m为\\033[0;31m{TF['准确率'] * 100:.2f}%\")\n",
    "# print(f\"\\033[0m测试集\\033[0;34mf1值\\033[0m为\\033[0;31m{TF['f1'] * 100:.2f}%\")\n",
    "# print(f\"\\033[0m测试集\\033[0;34mrecall值\\033[0m为\\033[0;31m{TF['recall'] * 100:.2f}%\")\n",
    "# print(f\"\\033[0m测试集\\033[0;34mroc值\\033[0m为\\033[0;31m{TF['roc'] * 100:.2f}%\")\n",
    "# print(f\"\\033[0m测试集\\033[0;34mprecision值\\033[0m为\\033[0;31m{TF['precision'] * 100:.2f}%\\033[0m\")\n",
    "# print(f\"\\033[0m测试集\\033[0;34mf1_macro值\\033[0m为\\033[0;31m{TF['f1_macro'] * 100:.2f}%\")\n",
    "# print(f\"\\033[0m测试集\\033[0;34mrecall_macro值\\033[0m为\\033[0;31m{TF['recall_macro'] * 100:.2f}%\")\n",
    "# print(f\"\\033[0m测试集\\033[0;34mroc_macro值\\033[0m为\\033[0;31m{TF['roc_macro'] * 100:.2f}%\")\n",
    "# print(f\"\\033[0m测试集\\033[0;34mprecision_macro值\\033[0m为\\033[0;31m{TF['precision_macro'] * 100:.2f}%\\033[0m\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5fd1c21c-269c-4518-aa81-fa03913fea39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8345817\n",
      "8345817\n",
      "{0: 1312818, 1: 7032999}\n",
      "[0, 1]\n",
      "[1312818, 7032999]\n",
      "{0: 20104, 1: 8325713}\n",
      "[0, 1]\n",
      "[20104, 8325713]\n",
      "8345817\n",
      "[[9.99972632e-01 2.73675876e-05]\n",
      " [9.99892211e-01 1.07788643e-04]\n",
      " [9.99895329e-01 1.04671438e-04]\n",
      " ...\n",
      " [9.95886384e-01 4.11361567e-03]\n",
      " [9.99974933e-01 2.50666577e-05]\n",
      " [9.99866135e-01 1.33864500e-04]]\n"
     ]
    }
   ],
   "source": [
    "print(len(label_y))\n",
    "print(len(pred_y))\n",
    "get_res_(pred_y)\n",
    "get_res_(label_y)\n",
    "print(len(pred_y_p))\n",
    "print(pred_y_p)\n",
    "# print(np.where(pred_y_p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d7cf396e-d70f-49bf-b227-ae99c3ef1273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8345817\n",
      "8345817\n",
      "8345817\n"
     ]
    }
   ],
   "source": [
    "suspect_pool=[]\n",
    "s_label=[]\n",
    "print(len(domain_pool))\n",
    "print(len(domain_label))\n",
    "print(len(url_pool))\n",
    "for n,i in enumerate(pred_y):\n",
    "    if i==0:\n",
    "        suspect_pool.append(url_pool[n])\n",
    "        s_label.append(label_pool[n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "667321ce-c742-4f3b-8d93-87f67c50f630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1312818\n",
      "1312818\n"
     ]
    }
   ],
   "source": [
    "print(len(suspect_pool))\n",
    "print(len(s_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "debf0af5-f0b0-4cf5-a837-4b0b831a79af",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('suspect_pool_new.txt', 'w', encoding='utf-8') as f:\n",
    "    for i in range(len(suspect_pool)):\n",
    "        # 添加‘\\n’用于换行\n",
    "        f.write(suspect_pool[i]+\",\"+str(s_label[i])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "255d2606-28b0-46a6-b5c2-a59a67bb889f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# domain_label_a =np.loadtxt(\"domain_label_large.txt\")\n",
    "domain_pool_a = np.loadtxt(\"domain_pool_test.txt\")\n",
    "url_pool = []\n",
    "with open('url_pool_test.txt', 'r', encoding='utf-8') as f:\n",
    "    for data in f:\n",
    "        url_pool.append(data[:-1])\n",
    "\n",
    "domain_pool_a = preprocessing.scale(domain_pool_a)\n",
    "pred_y = domain_svm_model.predict(domain_pool_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f6a07084-5338-4bf1-8a09-ed45a0a6ce7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1025767\n",
      "{0: 161381, 1: 864386}\n",
      "[0, 1]\n",
      "[161381, 864386]\n",
      "1025767\n"
     ]
    }
   ],
   "source": [
    "print(len(pred_y))\n",
    "get_res_(pred_y)\n",
    "print(len(url_pool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b1d47ebd-4986-4d03-81f1-53ab8c85b170",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_susp=[]\n",
    "for i in range(len(pred_y)):\n",
    "    if pred_y[i]==0:\n",
    "        url_susp.append(url_pool[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8d5a4e43-ac88-49b4-abbe-22c579490ab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161381\n"
     ]
    }
   ],
   "source": [
    "print(len(url_susp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e5ae80e1-90c2-4e96-b98a-89c3eedc2ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('url_suspect_test.txt', 'w+', encoding='utf-8') as f:\n",
    "#     f.write(\"\\n\".join(url_susp))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffec2fc-ca3b-4513-ab1d-7cc6e66610c8",
   "metadata": {},
   "source": [
    "## 换用SVM，暂时废弃"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d3cfdabf-5858-4521-b4d6-9acbad40c2fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]......\n",
      "Warning: using -h 0 may be faster\n",
      "*....\n",
      "Warning: using -h 0 may be faster\n",
      "*.\n",
      "Warning: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 10276\n",
      "obj = -4831.333110, rho = -0.133689\n",
      "nSV = 5157, nBSV = 4933\n",
      "Total nSV = 5157\n",
      "训练模型完成\n",
      "测试集\u001b[0;34m准确率\u001b[0m为\u001b[0;31m94.90%\n",
      "\u001b[0m测试集\u001b[0;34mf1值\u001b[0m为\u001b[0;31m95.62%\n",
      "\u001b[0m测试集\u001b[0;34mrecall值\u001b[0m为\u001b[0;31m92.85%\n",
      "\u001b[0m测试集\u001b[0;34mroc值\u001b[0m为\u001b[0;31m95.41%\n",
      "\u001b[0m测试集\u001b[0;34mprecision值\u001b[0m为\u001b[0;31m98.55%\u001b[0m\n",
      "\u001b[0m测试集\u001b[0;34mf1_macro值\u001b[0m为\u001b[0;31m94.76%\n",
      "\u001b[0m测试集\u001b[0;34mrecall_macro值\u001b[0m为\u001b[0;31m95.41%\n",
      "\u001b[0m测试集\u001b[0;34mroc_macro值\u001b[0m为\u001b[0;31m95.41%\n",
      "\u001b[0m测试集\u001b[0;34mprecision_macro值\u001b[0m为\u001b[0;31m94.36%\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# 建立模型\n",
    "\n",
    "domain_svm_model = svm.SVC(C=1, kernel='rbf',verbose=True)\n",
    "\n",
    "randomSeed = 1\n",
    "domain_size = len(domain_pool)\n",
    "domain_pool = preprocessing.scale(domain_pool)\n",
    "domain_pool, domain_label = sklearn.utils.shuffle(domain_pool, domain_label, random_state=randomSeed)\n",
    "\n",
    "# 训练模型,x为训练集,y为标签\n",
    "domain_svm_model.fit(domain_pool[:int(domain_size * 0.8)], domain_label[:int(domain_size * 0.8)])\n",
    "print(\"训练模型完成\")\n",
    "\n",
    "pred_y = domain_svm_model.predict(domain_pool[int(domain_size * 0.8):])\n",
    "label_y = domain_label[int(domain_size * 0.8):]\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "timestamp = datetime.datetime.timestamp(now)\n",
    "\n",
    "TF = {\"timestamp\":timestamp,\n",
    "      \"randomSeed\":randomSeed,\n",
    "      \"训练集规模\":int(domain_size * 0.8),\n",
    "      \"测试集规模\":domain_size - int(domain_size * 0.8),\n",
    "      \"标准化均值\":tuple(domain_pool.mean(axis=0)),\n",
    "      \"标准化方差\":tuple(domain_pool.std(axis=0)),\n",
    "      \"准确率\": accuracy_score(label_y, pred_y),\n",
    "      \"f1\": f1_score(label_y, pred_y),\n",
    "      \"f1_macro\": f1_score(label_y, pred_y,average=\"macro\"),\n",
    "      \"recall_macro\": recall_score(label_y, pred_y,average=\"macro\"),\n",
    "      \"roc_macro\": roc_auc_score(label_y, pred_y,average=\"macro\"),\n",
    "      \"precision_macro\": precision_score(label_y, pred_y,average=\"macro\"),\n",
    "      \"recall\": recall_score(label_y, pred_y),\n",
    "      \"roc\": roc_auc_score(label_y, pred_y),\n",
    "      \"precision\": precision_score(label_y, pred_y),\n",
    "      \"模型名称\": \"SVM\"\n",
    "      }\n",
    "\n",
    "# with open(\"log_\"+TF[\"模型名称\"]+\".json\",\"w\") as f:\n",
    "#     b = json.dumps(TF)\n",
    "#     f.write(b)\n",
    "\n",
    "print(f\"测试集\\033[0;34m准确率\\033[0m为\\033[0;31m{TF['准确率'] * 100:.2f}%\")\n",
    "print(f\"\\033[0m测试集\\033[0;34mf1值\\033[0m为\\033[0;31m{TF['f1'] * 100:.2f}%\")\n",
    "print(f\"\\033[0m测试集\\033[0;34mrecall值\\033[0m为\\033[0;31m{TF['recall'] * 100:.2f}%\")\n",
    "print(f\"\\033[0m测试集\\033[0;34mroc值\\033[0m为\\033[0;31m{TF['roc'] * 100:.2f}%\")\n",
    "print(f\"\\033[0m测试集\\033[0;34mprecision值\\033[0m为\\033[0;31m{TF['precision'] * 100:.2f}%\\033[0m\")\n",
    "print(f\"\\033[0m测试集\\033[0;34mf1_macro值\\033[0m为\\033[0;31m{TF['f1_macro'] * 100:.2f}%\")\n",
    "print(f\"\\033[0m测试集\\033[0;34mrecall_macro值\\033[0m为\\033[0;31m{TF['recall_macro'] * 100:.2f}%\")\n",
    "print(f\"\\033[0m测试集\\033[0;34mroc_macro值\\033[0m为\\033[0;31m{TF['roc_macro'] * 100:.2f}%\")\n",
    "print(f\"\\033[0m测试集\\033[0;34mprecision_macro值\\033[0m为\\033[0;31m{TF['precision_macro'] * 100:.2f}%\\033[0m\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f5963937-2a82-492d-837b-6540f093c463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# domain_label = np.loadtxt(\"domain_label.txt\")\n",
    "# domain_pool = np.loadtxt(\"domain_pool.txt\")\n",
    "\n",
    "\n",
    "# pred_y = domain_svm_model.predict(domain_pool[int(domain_size * 0.8):])\n",
    "# label_y = domain_label[int(domain_size * 0.8):]\n",
    "\n",
    "# now = datetime.datetime.now()\n",
    "# timestamp = datetime.datetime.timestamp(now)\n",
    "\n",
    "# TF = {\"timestamp\":timestamp,\n",
    "#       \"randomSeed\":randomSeed,\n",
    "#       \"训练集规模\":int(domain_size * 0.8),\n",
    "#       \"测试集规模\":domain_size - int(domain_size * 0.8),\n",
    "#       \"标准化均值\":tuple(domain_pool.mean(axis=0)),\n",
    "#       \"标准化方差\":tuple(domain_pool.std(axis=0)),\n",
    "#       \"准确率\": accuracy_score(label_y, pred_y),\n",
    "#       \"f1\": f1_score(label_y, pred_y),\n",
    "#       \"f1_macro\": f1_score(label_y, pred_y,average=\"macro\"),\n",
    "#       \"recall_macro\": recall_score(label_y, pred_y,average=\"macro\"),\n",
    "#       \"roc_macro\": roc_auc_score(label_y, pred_y,average=\"macro\"),\n",
    "#       \"precision_macro\": precision_score(label_y, pred_y,average=\"macro\"),\n",
    "#       \"recall\": recall_score(label_y, pred_y),\n",
    "#       \"roc\": roc_auc_score(label_y, pred_y),\n",
    "#       \"precision\": precision_score(label_y, pred_y),\n",
    "#       \"模型名称\": \"SVM-all\"\n",
    "#       }\n",
    "\n",
    "# with open(\"log_\"+TF[\"模型名称\"]+\".json\",\"w\") as f:\n",
    "#     b = json.dumps(TF)\n",
    "#     f.write(b)\n",
    "\n",
    "# print(f\"测试集\\033[0;34m准确率\\033[0m为\\033[0;31m{TF['准确率'] * 100:.2f}%\")\n",
    "# print(f\"\\033[0m测试集\\033[0;34mf1值\\033[0m为\\033[0;31m{TF['f1'] * 100:.2f}%\")\n",
    "# print(f\"\\033[0m测试集\\033[0;34mrecall值\\033[0m为\\033[0;31m{TF['recall'] * 100:.2f}%\")\n",
    "# print(f\"\\033[0m测试集\\033[0;34mroc值\\033[0m为\\033[0;31m{TF['roc'] * 100:.2f}%\")\n",
    "# print(f\"\\033[0m测试集\\033[0;34mprecision值\\033[0m为\\033[0;31m{TF['precision'] * 100:.2f}%\\033[0m\")\n",
    "# print(f\"\\033[0m测试集\\033[0;34mf1_macro值\\033[0m为\\033[0;31m{TF['f1_macro'] * 100:.2f}%\")\n",
    "# print(f\"\\033[0m测试集\\033[0;34mrecall_macro值\\033[0m为\\033[0;31m{TF['recall_macro'] * 100:.2f}%\")\n",
    "# print(f\"\\033[0m测试集\\033[0;34mroc_macro值\\033[0m为\\033[0;31m{TF['roc_macro'] * 100:.2f}%\")\n",
    "# print(f\"\\033[0m测试集\\033[0;34mprecision_macro值\\033[0m为\\033[0;31m{TF['precision_macro'] * 100:.2f}%\\033[0m\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05960bcc-c607-43fb-a23c-e20cca77467f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0ac64c37-365f-4c42-8a71-7e56a71d67d7",
   "metadata": {},
   "source": [
    "# 处理test（第一阶段）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "30c74e32-5d16-4b6f-ad49-380f9a08a5ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已读取分析100000个数据\n",
      "已读取分析200000个数据\n",
      "已读取分析300000个数据\n",
      "已读取分析400000个数据\n",
      "已读取分析500000个数据\n",
      "已读取分析600000个数据\n",
      "已读取分析700000个数据\n",
      "已读取分析800000个数据\n",
      "已读取分析900000个数据\n",
      "已读取分析1000000个数据\n",
      "读取数据完成\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import re, collections, math, datetime\n",
    "import json\n",
    "import collections\n",
    "\n",
    "import sklearn\n",
    "import sklearn.svm as svm\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import *\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def getDomainFeature(domain: str):\n",
    "    feature = [domain.count('.'), len(domain), sum(c.isdigit() for c in domain), domain.count('-')]\n",
    "\n",
    "    count_c = 0\n",
    "    special_characters = (':', ';', '#', '!', '%', '~', '+', '_', '?', '=', '&', '[', ']')\n",
    "    for c in domain:\n",
    "        if c in special_characters:\n",
    "            count_c = count_c + 1\n",
    "    feature.append(count_c)\n",
    "\n",
    "    s = len(domain)\n",
    "    dd = collections.defaultdict(int)\n",
    "    for c in domain:\n",
    "        dd[c] += 1\n",
    "\n",
    "    # 字符随机性\n",
    "    # H(d) = － ∑lg( P( Xi ) ) * P( Xi )\n",
    "    feature.append(sum(map(lambda value: -(value / s) * math.log2(value / s), dd.values())))\n",
    "\n",
    "    # 元音字母比例\n",
    "    feature.append(sum(map(lambda x: dd[x], ('a', 'e', 'i', 'o', 'u'))) / s)\n",
    "\n",
    "    # 唯一字符数\n",
    "    feature.append(len(dd))\n",
    "\n",
    "    return feature\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ip_pool = []\n",
    "    # ip_label = []\n",
    "\n",
    "    domain_pool = []\n",
    "    # domain_label = []\n",
    "    url_pool = []\n",
    "\n",
    "    n = 0\n",
    "    count = 0\n",
    "    path1 = r\"中国大学生服务外包创新创业大赛/网址自动分类识别/test(unlabeled).csv\"\n",
    "    for _ in csv.reader(open(path1)):\n",
    "        try:\n",
    "            if re.match(r\"^\\d+\\.\\d+\\.\\d+\\.\\d+(:\\d+)?$\", _[0]):\n",
    "                # print(1)\n",
    "                ip_pool.append(_[0])\n",
    "                # ip_label.append(int(_[1] == '0'))\n",
    "            else:\n",
    "                # if count > 30000:\n",
    "                #     break\n",
    "                # domain_label.append(int(_[1] == '0'))\n",
    "                domain_pool.append(getDomainFeature(_[0]))\n",
    "                url_pool.append(_[0])\n",
    "                # if _[1] == '0':\n",
    "                #     count+=1\n",
    "        except:\n",
    "            print(\",1\",_[0])\n",
    "\n",
    "        n += 1\n",
    "        if n % 100000 == 0:\n",
    "            # break\n",
    "            print(f\"已读取分析{n}个数据\")\n",
    "            \n",
    "            \n",
    "    # path2 = \"data1add.csv\"\n",
    "    # for _ in csv.reader(open(path2)):\n",
    "    #     try:\n",
    "    #         if re.match(r\"^\\d+\\.\\d+\\.\\d+\\.\\d+(:\\d+)?$\", _[0]):\n",
    "    #             ip_pool.append(getIPFeature(_[0]))\n",
    "    #             # ip_label.append(int(_[1] == '0'))\n",
    "    #         else:\n",
    "    #             # domain_label.append(int(_[1] == '0'))\n",
    "    #             domain_pool.append(getDomainFeature(_[0]))\n",
    "    #             url_pool.append(_[0])\n",
    "    #     except:\n",
    "    #         print(\"1\",_)\n",
    "\n",
    "        # n += 1\n",
    "        # if n % 100000 == 0:\n",
    "        #     # break\n",
    "        #     print(f\"已读取分析{n}个数据\")\n",
    "    \n",
    "    print(\"读取数据完成\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bba46332-e281-487f-8ec0-ff168da23a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# np.savetxt(\"domain_pool_test.txt\",np.array(domain_pool))\n",
    "# with open(\"url_pool_test.txt\",\"w+\",encoding=\"UTF-8\") as f:\n",
    "#     f.write(\"\\n\".join(url_pool))\n",
    "# # np.savetxt(\"url_pool_test.txt\",np.array(url_pool))\n",
    "# with open(\"ip_pool_test.txt\",\"w+\",encoding=\"UTF-8\") as f:\n",
    "#     f.write(\"\\n\".join(ip_pool))\n",
    "# # np.savetxt(\"ip_pool_test.txt\",np.array(ip_pool))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417a7570-2fe1-4888-90c1-0173280fff55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch 1.8",
   "language": "python",
   "name": "torch1.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
